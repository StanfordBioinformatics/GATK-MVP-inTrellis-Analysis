{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trellis Neo4j: Job based Analysis\n",
    "\n",
    "#### V1.0\n",
    "##### - Temporarily skip to collect GATK subtasks informaion because the query takes more than 1 hour. \n",
    "\n",
    "#### V1.1\n",
    "##### - Exclue QC job information in Bigquery \n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install py2neo for querying Neo4J "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install py2neo\n",
    "\n",
    "# #add python path of py2neo in system\n",
    "!pip3 install neotime\n",
    "!pip3 install neobolt\n",
    "!pip3 install pandas-gbq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "from google.cloud import storage\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Neo4J DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "bucket_info=''\n",
    "credential_info=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 1 : Read DB and Account Information in Google Storage (YAML)\n",
    "\n",
    "# create storage client\n",
    "storage_client = storage.Client()\n",
    "# get bucket with name\n",
    "bucket = storage_client.get_bucket(bucket_info)\n",
    "# get bucket data as blob\n",
    "blob = bucket.get_blob(credential_info)\n",
    "# convert to string\n",
    "yaml_data = blob.download_as_string()\n",
    "\n",
    "account = yaml.load(yaml_data, Loader=yaml.FullLoader)\n",
    "\n",
    "## Main Account\n",
    "graph = Graph(account['NEO4J_SCHEME']+'://'+account['NEO4J_HOST']+\":\"+str(account['NEO4J_PORT']), auth=(account['NEO4J_USER'],account['NEO4J_PASSPHRASE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## FQ2U Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FQ2U table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "#query = \"Match (fu:Job:FastqToUbam) RETURN fu.sample AS sample, fu.readGroup AS fq2urg_gatkid, fu.duplicate AS dup, fu.machineType AS vm_type, fu.durationMinutes as job_runtime\"\n",
    "#job_fq2u = graph.run(query).to_data_frame()\n",
    "#job_fq2u.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query\n",
    "query = \"Match (fu:Job:FastqToUbam)-[:STATUS]->(s:Dstat) RETURN fu.sample AS sample, fu.readGroup AS fq2urg_gatkid, fu.duplicate AS dup, fu.machineType AS vm_type, fu.durationMinutes as job_runtime, fu.startTimeEpoch as start_time, fu.stopTimeEpoch as stop_time, s.status as dstat_status, s.statusMessage as dstat_msg, s.logging as dstat_log\"\n",
    "job_fq2u = graph.run(query).to_data_frame()\n",
    "job_fq2u.set_index('sample')\n",
    "\n",
    "## Variable\n",
    "num_fq2u_sample=len(job_fq2u['sample'].unique())\n",
    "num_fq2u_job=len(job_fq2u)\n",
    "\n",
    "## Print (Info)\n",
    "print(\"The number of samples with FQ2U jobs : \" + str(num_fq2u_sample))\n",
    "print(\"The number of FQ2U jobs : \" + str(num_fq2u_job))\n",
    "\n",
    "## Bigquery Table Format\n",
    "job_fq2u['job_group']='GATK'\n",
    "job_fq2u['vm_exp_cnt']=1\n",
    "job_fq2u['job']='FQ2U'\n",
    "job_fq2u['vm_cnt']=1\n",
    "job_fq2u['vm_disk']=None\n",
    "job_fq2u['vm_avg_runtime']=job_fq2u['job_runtime']\n",
    "columnlist=['sample','job_group','job','fq2urg_gatkid','dup','vm_exp_cnt','vm_cnt','vm_avg_runtime','job_runtime','vm_type','vm_disk','start_time','stop_time','dstat_status','dstat_msg','dstat_log']\n",
    "job_fq2u=job_fq2u[columnlist]\n",
    "\n",
    "#display(job_fq2u.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FQ2U Duplication Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query\n",
    "fq2u_dup=job_fq2u.loc[job_fq2u['dup']==True,:]\n",
    "\n",
    "## Variable\n",
    "num_dup_fq2u_sample=len(fq2u_dup['sample'].unique())\n",
    "num_dup_fq2u_job=len(fq2u_dup)\n",
    "\n",
    "print(\"The number(percentage) of samples with duplicated FQ2U jobs : \" + str(len(fq2u_dup['sample'].unique()))+\" (\"+'{:2f}'.format((num_dup_fq2u_sample/num_fq2u_sample)*100)+\"%)\")\n",
    "print(\"The number(percentage) of FQ2U duplicated jobs : \" + str(num_dup_fq2u_job)+\" (\"+'{:2f}'.format((num_dup_fq2u_job/num_fq2u_job)*100)+\"%)\")\n",
    "\n",
    "#display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## GATK Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATK table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query\n",
    "query = \"MATCH (j:Job:CromwellWorkflow)-[:STATUS]->(s:Dstat) RETURN j.sample AS sample, j.cromwellWorkflowId AS fq2urg_gatkid, \\\n",
    "j.duplicate AS dup, j.durationMinutes as job_runtime, j.machineType as vm_type, j.startTimeEpoch as start_time, j.stopTimeEpoch as stop_time, s.status as dstat_status, s.statusMessage as dstat_msg, s.logging as dstat_log\"\n",
    "job_gatk = graph.run(query).to_data_frame()\n",
    "job_gatk.set_index('sample')\n",
    "\n",
    "## Variable\n",
    "num_gatk_sample=len(job_gatk['sample'].unique())\n",
    "num_gatk_job=len(job_gatk)\n",
    "\n",
    "## Print (Info)\n",
    "print(\"The number of samples with GATK jobs : \" + str(num_gatk_sample))\n",
    "print(\"The number of GATK jobs : \" + str(num_gatk_job))\n",
    "\n",
    "## Bigquery Table Format\n",
    "job_gatk['job_group']='GATK'\n",
    "job_gatk['vm_exp_cnt']=1\n",
    "job_gatk['job']='cromwell'\n",
    "job_gatk['vm_cnt']=1\n",
    "job_gatk['vm_avg_runtime']=job_gatk['job_runtime']\n",
    "job_gatk['vm_disk']=None\n",
    "columnlist=['sample','job_group','job','fq2urg_gatkid','dup','vm_exp_cnt','vm_cnt','vm_avg_runtime','job_runtime','vm_type','vm_disk','start_time','stop_time','dstat_status','dstat_msg','dstat_log']\n",
    "job_gatk=job_gatk[columnlist]\n",
    "\n",
    "#display(job_gatk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATK Duplication Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query\n",
    "gatk_dup=job_gatk.loc[job_gatk['dup']==True,:]\n",
    "\n",
    "## Variable\n",
    "num_dup_gatk_sample=len(gatk_dup['sample'].unique())\n",
    "num_dup_gatk_job=len(gatk_dup)\n",
    "\n",
    "print(\"The number(percentage) of samples with duplicated GATK jobs : \" + str(len(gatk_dup['sample'].unique()))+\" (\"+'{:2f}'.format((num_dup_gatk_sample/num_gatk_sample)*100)+\"%)\")\n",
    "print(\"The number(percentage) of GATK duplicated jobs : \" + str(num_dup_gatk_job)+\" (\"+'{:2f}'.format((num_dup_gatk_job/num_gatk_job)*100)+\"%)\")\n",
    "\n",
    "#display(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATK vm_exp_cnt and add_vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## expected vm data frame\n",
    "#vm_exp_cnt_df=pd.read_excel(\"./GATKstep_expected_vm.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_vm_cnt_df=pd.merge(vm_exp_cnt_df,job_gatk,left_on=['job'],right_on=['job'],how='right')\n",
    "#merged_attemps_df['added_vm']=job_gatk['vm_cnt']-merged_attemps_df['vm_exp_cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## GATK substeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vm_cnt table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Query\n",
    "# #query = \"MATCH (g:Job:CromwellWorkflow)-[:LED_TO*]->(s:CromwellStep)-[:HAS_ATTEMPT]-()-[*0..100]->(j:Job) \\\n",
    "# #WHERE g.cromwellWorkflowId=s.cromwellWorkflowId RETURN g.sample as sample, s.cromwellWorkflowId as fq2urg_gatkid, \\\n",
    "# #s.wdlCallAlias as job, count(distinct j) as vm_cnt, (max(j.stopTimeEpoch)-min(j.startTimeEpoch))/60 as job_runtime, avg(j.durationMinutes) as vm_avg_runtime, j.machineType as vm_type\"\n",
    "# query = \"MATCH (g:Job:CromwellWorkflow), (s:CromwellStep)-[:HAS_ATTEMPT]-()-[*0..100]->(j:Job) \\\n",
    "# WHERE g.cromwellWorkflowId=s.cromwellWorkflowId RETURN g.sample as sample, s.cromwellWorkflowId as fq2urg_gatkid, \\\n",
    "# s.wdlCallAlias as job, count(distinct j) as vm_cnt, (max(j.stopTimeEpoch)-min(j.startTimeEpoch))/60 as job_runtime, avg(j.durationMinutes) as vm_avg_runtime, j.machineType as vm_type\"\n",
    "# #query = \"MATCH (j:Job:CromwellWorkflow)-[:STATUS]->(s:Dstat) RETURN j.sample AS sample, j.duplicate AS dup, j.durationMinutes as job_runtime, s.status as dstat_status, s.statusMessage as dstat_msg, s.logging as dstat_log\"\n",
    "# job_gatk_step = graph.run(query).to_data_frame()\n",
    "# job_gatk_step.set_index('sample')\n",
    "\n",
    "# ## Variable\n",
    "# num_gatk_sample=len(job_gatk_step['sample'].unique())\n",
    "# num_gatk_subjobs=len(job_gatk_step)\n",
    "\n",
    "# ## Print (Info)\n",
    "# print(\"The number of samples with GATK steps : \" + str(num_gatk_sample))\n",
    "# print(\"The number of GATK subjobs : \" + str(num_gatk_subjobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATK Duplication Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Bigquery Table Format\n",
    "# job_gatk_step['job_group']='GATK'\n",
    "# job_gatk_step['vm_disk']=None\n",
    "\n",
    "# job_gatk_info=job_gatk[['sample','fq2urg_gatkid','dup','dstat_status','dstat_msg','dstat_log']]\n",
    "# job_gatk_stepm=pd.merge(job_gatk_info, job_gatk_step, left_on=['sample','fq2urg_gatkid'], right_on=['sample','fq2urg_gatkid'], how='right')\n",
    "\n",
    "# #display(job_gatk_stepm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATK vm_exp_cnt and add_vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## expected vm data frame\n",
    "# vm_exp_cnt_file = 'gs://'+account['TRELLIS_BUCKET']+'/analysis-notebooks/GATKstep_expected_vm.xlsx'\n",
    "# vm_exp_cnt_df=pd.read_excel(vm_exp_cnt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_gatk_stepm=pd.merge(vm_exp_cnt_df,job_gatk_stepm,left_on=['job'],right_on=['job'],how='right')\n",
    "# columnlist=['sample','job_group','job','fq2urg_gatkid','dup','vm_exp_cnt','vm_cnt','vm_avg_runtime','job_runtime','vm_type','vm_disk','dstat_status','dstat_msg','dstat_log']\n",
    "# job_gatk_stepm=job_gatk_stepm[columnlist]\n",
    "# #merged_attemps_df['added_vm']=job_gatk['vm_cnt']-merged_attemps_df['vm_exp_cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## QC Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"MATCH (j:Job:BamFastqc)-[:STATUS]->(d:Dstat) RETURN j.sample as sample, j.name as job, j.duplicate as dup, j.machineType as vm_type, j.durationMinutes as job_runtime, j.diskSize as vm_disk, j.startTimeEpoch as start_time, j.stopTimeEpoch as stop_time, d.status as dstat_status, d.statusMessage as dstat_msg, d.logging as dstat_log\"\n",
    "# job_fastqc = graph.run(query).to_data_frame()\n",
    "\n",
    "# job_fastqc['job_group']='QC'\n",
    "# job_fastqc['fq2urg_gatkid']=None\n",
    "# job_fastqc['vm_exp_cnt']=1\n",
    "# job_fastqc['vm_cnt']=1\n",
    "# job_fastqc['vm_avg_runtime']=job_fastqc['job_runtime']\n",
    "\n",
    "# job_fastqc=job_fastqc[columnlist]\n",
    "# job_fastqc.set_index('sample')\n",
    "\n",
    "# ## duplication check\n",
    "\n",
    "# print('The number of rows : ' + str(len(job_fastqc)))\n",
    "# print('The number of duplicated jobs : ' + str(len(job_fastqc.loc[job_fastqc['dup']==True,:])))\n",
    "\n",
    "# job_fastqc.drop_duplicates(columnlist,keep='first',inplace=True)\n",
    "\n",
    "# print('The number of rows dropped duplications : ' + str(len(job_fastqc)))\n",
    "\n",
    "# ## column order\n",
    "\n",
    "# columnlist=['sample','job_group','job','fq2urg_gatkid','dup','vm_exp_cnt','vm_cnt','vm_avg_runtime','job_runtime','vm_type','vm_disk','start_time','stop_time','dstat_status','dstat_msg','dstat_log']\n",
    "# job_fastqc=job_fastqc[columnlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text2table for Fastqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"MATCH (j:Job:BamFastqc)-[:OUTPUT]->()-[:INPUT_TO]->(t:Job:TextToTable)-[:STATUS]->(d:Dstat) RETURN j.sample as sample, j.name as fq2urg_gatkid, t.name as job, t.duplicate as dup, t.machineType as vm_type, t.durationMinutes as job_runtime, t.diskSize as vm_disk, t.startTimeEpoch as start_time, t.stopTimeEpoch as stop_time, d.status as dstat_status, d.statusMessage as dstat_msg, d.logging as dstat_log\"\n",
    "# job_fastqc_t2t=graph.run(query).to_data_frame()\n",
    "\n",
    "# job_fastqc_t2t['job_group']='QC'\n",
    "# job_fastqc_t2t['vm_exp_cnt']=1\n",
    "# job_fastqc_t2t['vm_cnt']=1\n",
    "# job_fastqc_t2t['vm_avg_runtime']=job_fastqc_t2t['job_runtime']\n",
    "\n",
    "# job_fastqc_t2t=job_fastqc_t2t[columnlist]\n",
    "# job_fastqc_t2t.set_index('sample')\n",
    "\n",
    "# ## duplication check\n",
    "\n",
    "# print('The number of rows : ' + str(len(job_fastqc_t2t)))\n",
    "# print('The number of duplicated jobs : ' + str(len(job_fastqc_t2t.loc[job_fastqc_t2t['dup']==True,:])))\n",
    "\n",
    "# ## drop duplication\n",
    "\n",
    "# job_fastqc_t2t.drop_duplicates(columnlist,keep='first',inplace=True)\n",
    "# print('The number of rows dropped duplications : ' + str(len(job_fastqc_t2t)))\n",
    "\n",
    "# ## column order\n",
    "\n",
    "# columnlist=['sample','job_group','job','fq2urg_gatkid','dup','vm_exp_cnt','vm_cnt','vm_avg_runtime','job_runtime','vm_type','vm_disk','start_time','stop_time','dstat_status','dstat_msg','dstat_log']\n",
    "# job_fastqc_t2t=job_fastqc_t2t[columnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_fastqc_t2t.loc[job_fastqc_t2t['sample']=='SHIP5119485','dstat_log'][196]\n",
    "#job_fastqc_t2t['sample'].value_counts()\n",
    "\n",
    "## Missing \n",
    "#set(job_fastqc['sample'])-set(job_fastqc_t2t['sample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flagstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"MATCH (s:Sample), (j:Job:Flagstat)-[:STATUS]->(d:Dstat) WHERE s.sample=j.sample RETURN s.sample as sample, j.name as job, j.duplicate as dup, j.machineType as vm_type, j.durationMinutes as job_runtime, j.diskSize as vm_disk, j.startTimeEpoch as start_time, j.stopTimeEpoch as stop_time, d.status as dstat_status, d.statusMessage as dstat_msg, d.logging as dstat_log\"\n",
    "# job_flagstat = graph.run(query).to_data_frame()\n",
    "\n",
    "# job_flagstat['job_group']='QC'\n",
    "# job_flagstat['fq2urg_gatkid']=None\n",
    "# job_flagstat['vm_exp_cnt']=1\n",
    "# job_flagstat['vm_cnt']=1\n",
    "# job_flagstat['vm_avg_runtime']=job_flagstat['job_runtime']\n",
    "\n",
    "# job_flagstat=job_flagstat[columnlist]\n",
    "# job_flagstat.set_index('sample')\n",
    "\n",
    "# print('The number of rows : ' + str(len(job_flagstat)))\n",
    "# print('The number of duplicated jobs : ' + str(len(job_flagstat.loc[job_flagstat['dup']==True,:])))\n",
    "\n",
    "# ## drop duplication\n",
    "\n",
    "# job_flagstat.drop_duplicates(columnlist,keep='first',inplace=True)\n",
    "# print('The number of rows dropped duplications : ' + str(len(job_flagstat)))\n",
    "\n",
    "# ## column order\n",
    "\n",
    "# columnlist=['sample','job_group','job','fq2urg_gatkid','dup','vm_exp_cnt','vm_cnt','vm_avg_runtime','job_runtime','vm_type','vm_disk','start_time','stop_time','dstat_status','dstat_msg','dstat_log']\n",
    "# job_flagstat=job_flagstat[columnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_flagstat.loc[job_flagstat['sample']=='SHIP5119453',:]\n",
    "#job_flagstat['sample'].value_counts()\n",
    "\n",
    "## Missing \n",
    "#set(job_fastqc['sample'])-set(job_fastqc_t2t['sample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text2table for Flagstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"MATCH (s:Sample), (j:Job:Flagstat)-[:OUTPUT]->()-[:INPUT_TO]->(t:Job:TextToTable)-[:STATUS]->(d:Dstat) WHERE s.sample=j.sample RETURN s.sample as sample, j.name as fq2urg_gatkid, t.name as job, t.duplicate as dup, t.machineType as vm_type, t.durationMinutes as job_runtime, t.diskSize as vm_disk, t.startTimeEpoch as start_time, t.stopTimeEpoch as stop_time, d.status as dstat_status, d.statusMessage as dstat_msg, d.logging as dstat_log\"\n",
    "# job_flagstat_t2t=graph.run(query).to_data_frame()\n",
    "\n",
    "# job_flagstat_t2t['job_group']='QC'\n",
    "# job_flagstat_t2t['vm_exp_cnt']=1\n",
    "# job_flagstat_t2t['vm_cnt']=1\n",
    "# job_flagstat_t2t['vm_avg_runtime']=job_flagstat_t2t['job_runtime']\n",
    "\n",
    "# job_flagstat_t2t=job_flagstat_t2t[columnlist]\n",
    "# job_flagstat_t2t.set_index('sample')\n",
    "\n",
    "# print('The number of rows : ' + str(len(job_flagstat_t2t)))\n",
    "# print('The number of duplicated jobs : ' + str(len(job_flagstat_t2t.loc[job_flagstat_t2t['dup']==True,:])))\n",
    "\n",
    "# ## drop duplication\n",
    "\n",
    "# job_flagstat_t2t.drop_duplicates(columnlist,keep='first',inplace=True)\n",
    "# print('The number of rows dropped duplications : ' + str(len(job_flagstat_t2t)))\n",
    "\n",
    "# columnlist=['sample','job_group','job','fq2urg_gatkid','dup','vm_exp_cnt','vm_cnt','vm_avg_runtime','job_runtime','vm_type','vm_disk','start_time','stop_time','dstat_status','dstat_msg','dstat_log']\n",
    "# job_flagstat_t2t=job_flagstat_t2t[columnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_flagstat_t2t.loc[job_flagstat_t2t['sample']=='SHIP5119453',:]\n",
    "#job_flagstat_t2t['sample'].value_counts()\n",
    "\n",
    "## Missing \n",
    "#set(job_flagstat['sample'])-set(job_flagstat_t2t['sample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vcfstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"MATCH (s:Sample), (j:Job:Vcfstats)-[:STATUS]->(d:Dstat) WHERE s.sample=j.sample RETURN s.sample as sample, j.name as job, j.duplicate as dup, j.machineType as vm_type, j.durationMinutes as job_runtime, j.diskSize as vm_disk, j.startTimeEpoch as start_time, j.stopTimeEpoch as stop_time, d.status as dstat_status, d.statusMessage as dstat_msg, d.logging as dstat_log\"\n",
    "# job_vcfstats = graph.run(query).to_data_frame()\n",
    "\n",
    "# job_vcfstats['job_group']='QC'\n",
    "# job_vcfstats['fq2urg_gatkid']=None\n",
    "# job_vcfstats['vm_exp_cnt']=1\n",
    "# job_vcfstats['vm_cnt']=1\n",
    "# job_vcfstats['vm_avg_runtime']=job_vcfstats['job_runtime']\n",
    "\n",
    "# job_vcfstats=job_vcfstats[columnlist]\n",
    "# job_vcfstats.set_index('sample')\n",
    "\n",
    "# print('The number of rows : ' + str(len(job_vcfstats)))\n",
    "# print('The number of duplicated jobs : ' + str(len(job_vcfstats.loc[job_vcfstats['dup']==True,:])))\n",
    "\n",
    "# ## drop duplication\n",
    "\n",
    "# job_vcfstats.drop_duplicates(columnlist,keep='first',inplace=True)\n",
    "# print('The number of rows dropped duplications : ' + str(len(job_vcfstats)))\n",
    "\n",
    "# columnlist=['sample','job_group','job','fq2urg_gatkid','dup','vm_exp_cnt','vm_cnt','vm_avg_runtime','job_runtime','vm_type','vm_disk','start_time','stop_time','dstat_status','dstat_msg','dstat_log']\n",
    "# job_vcfstats=job_vcfstats[columnlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text2table for Vcfstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"MATCH (s:Sample), (j:Job:Vcfstats)-[:OUTPUT]->()-[:INPUT_TO]->(t:Job:TextToTable)-[:STATUS]->(d:Dstat) WHERE s.sample=j.sample RETURN s.sample as sample, j.name as fq2urg_gatkid, t.name as job, t.duplicate as dup, t.machineType as vm_type, t.durationMinutes as job_runtime, t.diskSize as vm_disk, t.startTimeEpoch as start_time, t.stopTimeEpoch as stop_time, d.status as dstat_status, d.statusMessage as dstat_msg, d.logging as dstat_log\"\n",
    "# job_vcfstats_t2t=graph.run(query).to_data_frame()\n",
    "\n",
    "# job_vcfstats_t2t['job_group']='QC'\n",
    "# job_vcfstats_t2t['vm_exp_cnt']=1\n",
    "# job_vcfstats_t2t['vm_cnt']=1\n",
    "# job_vcfstats_t2t['vm_avg_runtime']=job_vcfstats_t2t['job_runtime']\n",
    "\n",
    "# job_vcfstats_t2t=job_vcfstats_t2t[columnlist]\n",
    "# job_vcfstats_t2t.set_index('sample')\n",
    "\n",
    "# print('The number of rows : ' + str(len(job_vcfstats_t2t)))\n",
    "# print('The number of duplicated jobs : ' + str(len(job_vcfstats_t2t.loc[job_vcfstats_t2t['dup']==True,:])))\n",
    "\n",
    "# ## drop duplication\n",
    "\n",
    "# job_vcfstats_t2t.drop_duplicates(columnlist,keep='first',inplace=True)\n",
    "# print('The number of rows dropped duplications : ' + str(len(job_vcfstats_t2t)))\n",
    "\n",
    "# columnlist=['sample','job_group','job','fq2urg_gatkid','dup','vm_exp_cnt','vm_cnt','vm_avg_runtime','job_runtime','vm_type','vm_disk','start_time','stop_time','dstat_status','dstat_msg','dstat_log']\n",
    "# job_vcfstats_t2t=job_vcfstats_t2t[columnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_flagstat_t2t.loc[job_flagstat_t2t['sample']=='SHIP5119453',:]\n",
    "#job_vcfstats_t2t['sample'].value_counts()\n",
    "\n",
    "## Missing \n",
    "#set(job_vcfstats['sample'])-set(job_vcfstats_t2t['sample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## v1.0\n",
    "\n",
    "# #job_df=pd.concat([job_fq2u,job_gatk,job_gatk_stepm,job_fastqc,job_fastqc_t2t,job_flagstat,job_flagstat_t2t,job_vcfstats,job_vcfstats_t2t]).sort_values(['sample','job_group','job','fq2urg_gatkid'])\n",
    "# job_df=pd.concat([job_fq2u,job_gatk,job_fastqc,job_fastqc_t2t,job_flagstat,job_flagstat_t2t,job_vcfstats,job_vcfstats_t2t]).sort_values(['sample','job_group','job','fq2urg_gatkid'])\n",
    "# display(job_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## v1.1\n",
    "\n",
    "#job_df=pd.concat([job_fq2u,job_gatk,job_gatk_stepm,job_fastqc,job_fastqc_t2t,job_flagstat,job_flagstat_t2t,job_vcfstats,job_vcfstats_t2t]).sort_values(['sample','job_group','job','fq2urg_gatkid'])\n",
    "job_df=pd.concat([job_fq2u,job_gatk]).sort_values(['sample','job_group','job','fq2urg_gatkid'])\n",
    "display(job_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vm_avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom CPU cost : 0.033174/CPU/Hour, # Custom Mem cost : 0.004446/GB/Hour, # Custom Disk cost : ???\n",
    "cpu_sd_cost = 0.033174\n",
    "memg_sd_cost = 0.004446\n",
    "\n",
    "cpu_pem_cost = 0.00698\n",
    "memg_pem_cost = 0.00094\n",
    "\n",
    "## Extract cpu and mem info.\n",
    "#temp=job_df.loc[:,['job','vm_type']]\n",
    "job_df.loc[:,'vm_std']=None\n",
    "job_df.loc[:,'vm_cpu']=0\n",
    "job_df.loc[:,'vm_mem']=0\n",
    "job_df[['vm_std','vm_cpu','vm_mem']]=[i.split('-') for i in job_df['vm_type']]\n",
    "job_df['vm_cpu']=[int(x) for x in job_df['vm_cpu']]\n",
    "job_df['vm_mem']=[int(x) for x in job_df['vm_mem']]\n",
    "#columnlist=['job','vm_type','vm_type','cpu','mem']\n",
    "#temp=temp[columnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FQ2U and GATK job unit cost with Standard VM\n",
    "job_df.loc[(job_df['job_group']=='GATK')&(job_df['job'].isin(['FQ2U','cromwell'])),'vm_avg_cost']= np.array(job_df.loc[(job_df['job_group']=='GATK')&(job_df['job'].isin(['FQ2U','cromwell'])),'vm_cpu'])*cpu_sd_cost/60 + np.array(job_df.loc[(job_df['job_group']=='GATK')&(job_df['job'].isin(['FQ2U','cromwell'])),'vm_mem'])*memg_sd_cost/60/1000\n",
    "\n",
    "# QC unit cost with Standard VM\n",
    "#job_df.loc[(job_df['job_group']=='QC'),'vm_avg_cost']= np.array(job_df.loc[(job_df['job_group']=='QC'),'vm_cpu'])*cpu_sd_cost/60 + np.array(job_df.loc[(job_df['job_group']=='QC'),'vm_mem'])*memg_sd_cost/60/1000\n",
    "\n",
    "# GATK sub jobs' unit cost with Preemptible VM\n",
    "#job_df.loc[(job_df['job_group']=='GATK')&(job_df['job'].isin(['FQ2U','cromwell'])==False),'vm_avg_cost']= np.array(job_df.loc[(job_df['job_group']=='GATK')&(job_df['job'].isin(['FQ2U','cromwell'])==False),'vm_cpu'])*cpu_pem_cost/60 + np.array(job_df.loc[(job_df['job_group']=='GATK')&(job_df['job'].isin(['FQ2U','cromwell'])==False),'vm_mem'])*memg_pem_cost/60/1000 \n",
    "\n",
    "job_df.loc[(job_df['job_group']=='GATK')&(job_df['job'].isin(['FQ2U','cromwell'])==False),'vm_avg_cost']= \\\n",
    "    np.array(job_df.loc[(job_df['job_group']=='GATK')&(job_df['job'].isin(['FQ2U','cromwell'])==False),'vm_cpu'])*cpu_pem_cost/60 + \\\n",
    "    np.array(job_df.loc[(job_df['job_group']=='GATK')&(job_df['job'].isin(['FQ2U','cromwell'])==False),'vm_mem'])*memg_pem_cost/60/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### job cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.8f' % x)\n",
    "\n",
    "## FQ2U\n",
    "job_df.loc[job_df['job']=='FQ2U','job_cost']=np.array(job_df.loc[job_df['job']=='FQ2U','job_runtime'])*np.array(job_df.loc[job_df['job']=='FQ2U','vm_avg_cost'])\n",
    "#merged_cost_df.head(2)\n",
    "\n",
    "## GATK\n",
    "job_df.loc[job_df['job']=='cromwell','job_cost']=np.array(job_df.loc[job_df['job']=='cromwell','job_runtime'])*np.array(job_df.loc[job_df['job']=='cromwell','vm_avg_cost'])\n",
    "#merged_cost_df[merged_cost_df['job']=='GATK'].head(2)\n",
    "\n",
    "## QC\n",
    "#job_df.loc[job_df['job_group']=='QC','job_cost']=np.array(job_df.loc[job_df['job_group']=='QC','job_runtime'])*np.array(job_df.loc[job_df['job_group']=='QC','vm_avg_cost'])\n",
    "#merged_cost_df[merged_cost_df['job']=='GATK'].head(2)\n",
    "\n",
    "## GATK steps\n",
    "job_df.loc[(job_df['job_group']=='GATK')&(job_df['job']!='FQ2U') & (job_df['job']!='cromwell'),'job_cost']=np.array(job_df.loc[(job_df['job_group']=='GATK')&(job_df['job']!='FQ2U') & (job_df['job']!='cromwell'),'vm_cnt']) \\\n",
    "*np.array(job_df.loc[(job_df['job_group']=='GATK')&(job_df['job']!='FQ2U') & (job_df['job']!='cromwell'),'vm_avg_runtime'])*np.array(job_df.loc[(job_df['job_group']=='GATK')&(job_df['job']!='FQ2U') & (job_df['job']!='cromwell'),'vm_avg_cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnlist=['sample','job_group','job','fq2urg_gatkid','dup','vm_exp_cnt','vm_cnt','vm_avg_runtime','job_runtime','vm_type','vm_cpu','vm_mem','vm_disk','vm_avg_cost','job_cost','start_time','stop_time','dstat_status','dstat_msg','dstat_log']\n",
    "job_df=job_df[columnlist]\n",
    "job_df.head()\n",
    "#job_df.to_csv('job-based-analysis-v1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload CSV Files to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id=account['BIGQUERY_DATASET']+'.job_based_analysis'\n",
    "projectid=account['GOOGLE_CLOUD_PROJECT']\n",
    "\n",
    "pandas_gbq.to_gbq(\n",
    "    job_df, table_id, project_id=projectid, if_exists='replace',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
